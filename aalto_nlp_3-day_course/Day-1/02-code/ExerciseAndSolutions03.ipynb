{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaG2QDP-y2k6"
      },
      "source": [
        "<center>\n",
        "    <h1> Natural Language Processing and Large Language Models for Research Data Exploration and Analysis\n",
        " </h1> </center>\n",
        "\n",
        "\n",
        "<center> <h1> Day-1: Text Classification and Sentiment Analysis using TextBlob </h1> </center>\n",
        "\n",
        "<center> <h2> Exercise - 02 (part - 02) </h2> </center>\n",
        "\n",
        "<center> <h4> Raghava Mukkamala (rrm.digi@cbs.dk)  </h4> </center>\n",
        "\n",
        "\n",
        "### Instructions\n",
        "\n",
        "#### Please use Python 3 for working on the following questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5feWE3Ny2k_"
      },
      "source": [
        "\n",
        "# Exercise 02: Text Classification using NaiveBayesClassifier using NLTK\n",
        "\n",
        "Source: https://www.nltk.org/book/ch06.html\n",
        "\n",
        "adapted by Raghava Mukkamala\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kyEafk2By2k_"
      },
      "outputs": [],
      "source": [
        "# !pip install prettytable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y3PeFzTcy2lC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "from prettytable import PrettyTable\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2cJDsCBy2lC",
        "outputId": "3dfac8b0-b1a8-4f10-96f0-a0666a0410d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['capsule', ':', 'the', 'best', 'place', 'to', 'start', ...]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('movie_reviews')\n",
        "\n",
        "# if you get error then you can download movie reviews by using\n",
        "# nltk.download('movie_reviews') and then unpack the downloaded zip file.\n",
        "\n",
        "print(movie_reviews.words('pos/cv957_8737.txt'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMfAFZqQy2lC",
        "outputId": "87714cae-55b3-4ad4-8745-c0df96298957"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg', 'pos']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "movie_reviews.categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "npXLQDHgy2lD",
        "outputId": "b1318028-9fba-47d1-a7ba-4fb3e07fa23c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.corpus.reader.util.StreamBackedCorpusView"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>nltk.corpus.reader.util.StreamBackedCorpusView</b><br/>def __init__(fileid, block_reader=None, startpos=0, encoding=&#x27;utf8&#x27;)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/nltk/corpus/reader/util.py</a>A &#x27;view&#x27; of a corpus file, which acts like a sequence of tokens:\n",
              "it can be accessed by index, iterated over, etc.  However, the\n",
              "tokens are only constructed as-needed -- the entire corpus is\n",
              "never stored in memory at once.\n",
              "\n",
              "The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
              "a corpus fileid (specified as a string or as a ``PathPointer``);\n",
              "and a block reader.  A &quot;block reader&quot; is a function that reads\n",
              "zero or more tokens from a stream, and returns them as a list.  A\n",
              "very simple example of a block reader is:\n",
              "\n",
              "    &gt;&gt;&gt; def simple_block_reader(stream):\n",
              "    ...     return stream.readline().split()\n",
              "\n",
              "This simple block reader reads a single line at a time, and\n",
              "returns a single token (consisting of a string) for each\n",
              "whitespace-separated substring on the line.\n",
              "\n",
              "When deciding how to define the block reader for a given\n",
              "corpus, careful consideration should be given to the size of\n",
              "blocks handled by the block reader.  Smaller block sizes will\n",
              "increase the memory requirements of the corpus view&#x27;s internal\n",
              "data structures (by 2 integers per block).  On the other hand,\n",
              "larger block sizes may decrease performance for random access to\n",
              "the corpus.  (But note that larger block sizes will *not*\n",
              "decrease performance for iteration.)\n",
              "\n",
              "Internally, ``CorpusView`` maintains a partial mapping from token\n",
              "index to file position, with one entry per block.  When a token\n",
              "with a given index *i* is requested, the ``CorpusView`` constructs\n",
              "it as follows:\n",
              "\n",
              "  1. First, it searches the toknum/filepos mapping for the token\n",
              "     index closest to (but less than or equal to) *i*.\n",
              "\n",
              "  2. Then, starting at the file position corresponding to that\n",
              "     index, it reads one block at a time using the block reader\n",
              "     until it reaches the requested token.\n",
              "\n",
              "The toknum/filepos mapping is created lazily: it is initially\n",
              "empty, but every time a new block is read, the block&#x27;s\n",
              "initial token is added to the mapping.  (Thus, the toknum/filepos\n",
              "map has one entry per block.)\n",
              "\n",
              "In order to increase efficiency for random access patterns that\n",
              "have high degrees of locality, the corpus view may cache one or\n",
              "more blocks.\n",
              "\n",
              ":note: Each ``CorpusView`` object internally maintains an open file\n",
              "    object for its underlying corpus file.  This file should be\n",
              "    automatically closed when the ``CorpusView`` is garbage collected,\n",
              "    but if you wish to close it manually, use the ``close()``\n",
              "    method.  If you access a ``CorpusView``&#x27;s items after it has been\n",
              "    closed, the file object will be automatically re-opened.\n",
              "\n",
              ":warning: If the contents of the file are modified during the\n",
              "    lifetime of the ``CorpusView``, then the ``CorpusView``&#x27;s behavior\n",
              "    is undefined.\n",
              "\n",
              ":warning: If a unicode encoding is specified when constructing a\n",
              "    ``CorpusView``, then the block reader may only call\n",
              "    ``stream.seek()`` with offsets that have been returned by\n",
              "    ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
              "    relative offsets, or with offsets based on string lengths, may\n",
              "    lead to incorrect behavior.\n",
              "\n",
              ":ivar _block_reader: The function used to read\n",
              "    a single block from the underlying file stream.\n",
              ":ivar _toknum: A list containing the token index of each block\n",
              "    that has been processed.  In particular, ``_toknum[i]`` is the\n",
              "    token index of the first token in block ``i``.  Together\n",
              "    with ``_filepos``, this forms a partial mapping between token\n",
              "    indices and file positions.\n",
              ":ivar _filepos: A list containing the file position of each block\n",
              "    that has been processed.  In particular, ``_toknum[i]`` is the\n",
              "    file position of the first character in block ``i``.  Together\n",
              "    with ``_toknum``, this forms a partial mapping between token\n",
              "    indices and file positions.\n",
              ":ivar _stream: The stream used to access the underlying corpus file.\n",
              ":ivar _len: The total number of tokens in the corpus, if known;\n",
              "    or None, if the number of tokens is not yet known.\n",
              ":ivar _eofpos: The character position of the last character in the\n",
              "    file.  This is calculated when the corpus view is initialized,\n",
              "    and is used to decide when the end of file has been reached.\n",
              ":ivar _cache: A cache of the most recently read block.  It\n",
              "   is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
              "   start_toknum is the token index of the first token in the block;\n",
              "   end_toknum is the token index of the first token not in the\n",
              "   block; and tokens is a list of the tokens in the block.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 32);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "type(movie_reviews.words('pos/cv957_8737.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVF2YEMvy2lD"
      },
      "source": [
        "## Loading and Transforming movie review documents\n",
        "\n",
        "    Load the documents from ../nltk_data/corpora/movie_reviews and\n",
        "    transform them in the following format.\n",
        "\n",
        "    [\n",
        "    ([ 'gotten', 'a', 'four', 'star', 'rating', 'out', 'of', 'me', '.'], 'pos'),\n",
        "    ([ 'free', 'tickets',  'definitely', 'worth', 'checking', 'out', '.'], 'pos')\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcIMVvVry2lD",
        "outputId": "c97af58c-c329-4bee-ffae-71edf8ab120c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movie_reviews.categories():  ['neg', 'pos']\n",
            "number of documents:  2000\n",
            "+----------------------------------------------------------------------------------+----------+\n",
            "|                                Document Features                                 | Category |\n",
            "+----------------------------------------------------------------------------------+----------+\n",
            "|     there,was,a,huge,crowd,-,so,many,over,100,people,could,not,be,admitted,-     |   pos    |\n",
            "| ,at,a,premiere,screening,of,\",the,nephew,\",(,first,screening,with,a,major,genera |          |\n",
            "| l,audience,admittance,),.,this,was,a,movie,premiere,at,the,santa,barbara,interna |          |\n",
            "|                          tional,film,festival,.,pierce                           |          |\n",
            "|                                                                                  |          |\n",
            "|                                                                                  |          |\n",
            "| rated,:,r,for,strong,language,,,sexual,dialogue,,,drug,use,,,crude,humor,,,viole |   pos    |\n",
            "| nce,and,brief,nudity,.,starring,:,ben,affleck,,,matt,damon,,,linda,fiorentino,,, |          |\n",
            "| salma,hayek,,,alan,rickman,,,chris,rock,,,kevin,smith,,,jason,mewes,,,jason,lee, |          |\n",
            "|                                        ,                                         |          |\n",
            "|                                                                                  |          |\n",
            "|                                                                                  |          |\n",
            "| after,a,rather,disappointing,\",mary,railly,\",,,stephen,frears,is,now,officially, |   pos    |\n",
            "| back,in,business,with,a,comeback,worthy,of,praise,.,this,is,one,of,the,best,surp |          |\n",
            "|                             rises,of,this,season,--                              |          |\n",
            "|          ,a,sweet,,,small,budget,comedy,with,a,big,heart,.,in,fact,it,'          |          |\n",
            "|                                                                                  |          |\n",
            "|                                                                                  |          |\n",
            "| i,have,no,real,tangible,proof,of,this,,,but,i,swear,that,there,are,a,lot,of,prod |   neg    |\n",
            "| ucers,in,hollywood,who,adamantly,believe,that,if,you,take,a,bad,script,,,written |          |\n",
            "|  ,by,a,bad,writer,and,give,the,project,it,to,an,equally,bad,director,,,then,it   |          |\n",
            "|                                                                                  |          |\n",
            "|                                                                                  |          |\n",
            "|     it,',s,always,a,bad,sign,when,the,core,audience,of,a,film,--,children,--     |   neg    |\n",
            "|                   ,are,either,walking,out,early,or,are,half,-                    |          |\n",
            "| ,asleep,when,the,credits,roll,at,the,end,of,a,film,.,that,about,sums,up,the,drea |          |\n",
            "|                       dful,ugliness,of,102,dalmatians,,,a                        |          |\n",
            "|                                                                                  |          |\n",
            "|                                                                                  |          |\n",
            "+----------------------------------------------------------------------------------+----------+\n"
          ]
        }
      ],
      "source": [
        "print ('movie_reviews.categories(): ', movie_reviews.categories())\n",
        "\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "    for category in movie_reviews.categories()\n",
        "    for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "print('number of documents: ', len(documents))\n",
        "\n",
        "tab = PrettyTable(['Document Features', 'Category'])\n",
        "\n",
        "tab.horizontal_char = '-'\n",
        "\n",
        "for (doc, cat) in documents[0:5]:\n",
        "    feats = textwrap.fill(','.join(doc[:50]), width=80)\n",
        "    tab.add_row([ feats, cat])\n",
        "    tab.add_row([ '\\n', '\\n'])\n",
        "#     print(cat)\n",
        "\n",
        "print(tab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvOneOTIy2lE"
      },
      "source": [
        "## Generate a Frequency distribution of words\n",
        "\n",
        "    Load all words from all the documents from the movie reviews to use\n",
        "    most common words as features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUnq87hIy2lE",
        "outputId": "c6abf21f-0083-40d7-d459-5a9f7a8a6212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words from movie review corpus:  1583820\n",
            "most freq words:  [('off', 1581), ('too', 1577), ('any', 1574), ('does', 1568), ('really', 1558), ('had', 1546), ('while', 1539), ('films', 1536), ('how', 1517), ('plot', 1513)]\n",
            "word_features[:25]:  [',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in', 's', '\"', 'it', 'that', '-', ')', '(', 'as', 'with', 'for', 'his', 'this', 'film', 'i', 'he']\n"
          ]
        }
      ],
      "source": [
        "print('total words from movie review corpus: ', len(movie_reviews.words()))\n",
        "\n",
        "# load all the words in freq distribution\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "\n",
        "most_freq_words = all_words.most_common(2000)\n",
        "\n",
        "print('most freq words: ', most_freq_words[100:110])\n",
        "\n",
        "word_features = [word for (word, count) in most_freq_words]\n",
        "\n",
        "print('word_features[:25]: ', word_features[:25])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SOgwJBby2lE"
      },
      "source": [
        "## Converting documents into training set containing features\n",
        "\n",
        "    Extarcting features from a document and transforming them feature sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VcW8IGyy2lE",
        "outputId": "90212b51-1fd2-4ba6-a889-2858ccc5e96d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformed document features, printing the first 25 features \n",
            "\n",
            " {'contains(,)': True, 'contains(the)': True, 'contains(.)': True, 'contains(a)': True, 'contains(and)': True, 'contains(of)': True, 'contains(to)': True, \"contains(')\": True, 'contains(is)': True, 'contains(in)': True, 'contains(s)': True, 'contains(\")': True, 'contains(it)': True, 'contains(that)': True, 'contains(-)': True, 'contains())': True, 'contains(()': True, 'contains(as)': True, 'contains(with)': True, 'contains(for)': True, 'contains(his)': True, 'contains(this)': True, 'contains(film)': False, 'contains(i)': False, 'contains(he)': True}\n"
          ]
        }
      ],
      "source": [
        "def get_document_features(document):\n",
        "    \"\"\"\n",
        "        This function will convert given document into a feature set.\n",
        "\n",
        "    \"\"\"\n",
        "    document_words = set(document)\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    for word in word_features:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "\n",
        "# test code for the above function\n",
        "\n",
        "words_doc = movie_reviews.words('pos/cv957_8737.txt')\n",
        "\n",
        "feat_dict = get_document_features(words_doc)\n",
        "\n",
        "feat_dict_25 = {k: feat_dict[k] for k in list(feat_dict.keys())[:25]}\n",
        "\n",
        "print('transformed document features, printing the first 25 features \\n\\n', feat_dict_25)\n",
        "\n",
        "# print(documents[1][1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LplWcwJry2lF"
      },
      "source": [
        "## Preparing training set and training Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ1hENt2y2lF",
        "outputId": "9b5bfb61-d1ef-4360-f863-1894f0a23534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "100\n",
            "accuracy:  0.8\n",
            "Most Informative Features\n",
            "   contains(outstanding) = True              pos : neg    =     11.0 : 1.0\n",
            "         contains(mulan) = True              pos : neg    =      9.0 : 1.0\n",
            "         contains(damon) = True              pos : neg    =      7.8 : 1.0\n",
            "        contains(seagal) = True              neg : pos    =      7.8 : 1.0\n",
            "   contains(wonderfully) = True              pos : neg    =      6.6 : 1.0\n",
            "        contains(poorly) = True              neg : pos    =      5.7 : 1.0\n",
            "        contains(wasted) = True              neg : pos    =      5.4 : 1.0\n",
            "          contains(lame) = True              neg : pos    =      5.1 : 1.0\n",
            "         contains(awful) = True              neg : pos    =      5.1 : 1.0\n",
            "           contains(era) = True              pos : neg    =      5.0 : 1.0\n",
            "         contains(flynt) = True              pos : neg    =      5.0 : 1.0\n",
            "    contains(ridiculous) = True              neg : pos    =      4.9 : 1.0\n",
            "         contains(waste) = True              neg : pos    =      4.8 : 1.0\n",
            "         contains(worst) = True              neg : pos    =      4.5 : 1.0\n",
            "        contains(allows) = True              pos : neg    =      4.5 : 1.0\n",
            "       contains(unfunny) = True              neg : pos    =      4.5 : 1.0\n",
            "         contains(bland) = True              neg : pos    =      4.4 : 1.0\n",
            "     contains(portrayal) = True              pos : neg    =      4.2 : 1.0\n",
            "          contains(mess) = True              neg : pos    =      4.2 : 1.0\n",
            "        contains(stupid) = True              neg : pos    =      4.1 : 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "featuresets = [(get_document_features(d), c) for (d,c) in documents]\n",
        "\n",
        "print(len(featuresets))\n",
        "\n",
        "\n",
        "train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "\n",
        "print(len(test_set))\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "\n",
        "print('accuracy: ', nltk.classify.accuracy(classifier, test_set))\n",
        "\n",
        "classifier.show_most_informative_features(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIgAWbQ-y2lF"
      },
      "source": [
        "## Testing the classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjDK_gD7y2lG",
        "outputId": "fd38ffd3-3c5c-47f4-8657-bf6e174d2c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result of sample review:  neg\n"
          ]
        }
      ],
      "source": [
        "sample_review = \"\"\"great.\"\"\"\n",
        "\n",
        "sample_review_doc_feats = get_document_features(sample_review.split())\n",
        "\n",
        "# print('Sample review features: \\n\\n',sample_review_doc_feats)\n",
        "\n",
        "print('result of sample review: ', classifier.classify(sample_review_doc_feats))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N_UNLVAy2lG"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## <font color='red'>Task - 02:</font>\n",
        "\n",
        "    Use Reuters Corpus from nltk and build a Naive Bayes classifier for the categories of Reuters Corpus.\n",
        "    Please refer to https://www.nltk.org/book/ch02.html for an example on how to access Reuters Corpus.\n",
        "    Use some test documents to test the accuracy of the classifier.\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure that NLTK and reuters corpus is accessible\n",
        "\n",
        "# If you get an error saying that 'Resource reuters not found.' ,\n",
        "# you can download using the following code\n",
        "\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# Check how many fields in the reuters corpus to see that we have access.\n",
        "print(len(reuters.fileids()))\n",
        "\n",
        "# Check how many fileids in the reuters corpus to see that we have access.\n",
        "print('number of documents: ', len(reuters.fileids()))\n",
        "\n",
        "print('Categories: \\n')\n",
        "print(reuters.categories())\n",
        "\n",
        "\n",
        "# Let's make a pretty table to look at the files and categories of the first 10 docs\n",
        "tab = PrettyTable(['fileid', 'Category'])\n",
        "\n",
        "index = 0\n",
        "\n",
        "print('printing the categories for first 20 docs!')\n",
        "\n",
        "for id in reuters.fileids():\n",
        "    index += 1\n",
        "    cats = textwrap.fill(','.join(reuters.categories(id)), width=40)\n",
        "    tab.add_row([id, cats])\n",
        "    if index == 20:\n",
        "        break\n",
        "\n",
        "print(tab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxU_N0syz638",
        "outputId": "0920ce59-df12-4ebf-fe68-8299c013bfa2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10788\n",
            "number of documents:  10788\n",
            "Categories: \n",
            "\n",
            "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
            "printing the categories for first 20 docs!\n",
            "+------------+------------------------------------------+\n",
            "|   fileid   |                 Category                 |\n",
            "+------------+------------------------------------------+\n",
            "| test/14826 |                  trade                   |\n",
            "| test/14828 |                  grain                   |\n",
            "| test/14829 |              crude,nat-gas               |\n",
            "| test/14832 |  corn,grain,rice,rubber,sugar,tin,trade  |\n",
            "| test/14833 |             palm-oil,veg-oil             |\n",
            "| test/14839 |                   ship                   |\n",
            "| test/14840 |  coffee,lumber,palm-oil,rubber,veg-oil   |\n",
            "| test/14841 |               grain,wheat                |\n",
            "| test/14842 |                   gold                   |\n",
            "| test/14843 |                   acq                    |\n",
            "| test/14844 |                   tin                    |\n",
            "| test/14849 |            interest,money-fx             |\n",
            "| test/14852 |                acq,copper                |\n",
            "| test/14854 |                   ipi                    |\n",
            "| test/14858 | carcass,corn,grain,livestock,oilseed,ric |\n",
            "|            |             e,soybean,trade              |\n",
            "| test/14859 |                   earn                   |\n",
            "| test/14860 |                   earn                   |\n",
            "| test/14861 |            interest,money-fx             |\n",
            "| test/14862 |                bop,trade                 |\n",
            "| test/14863 |                 gas,lead                 |\n",
            "+------------+------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the total numebr of words\n",
        "print('total number of words in the reuter corpus: ', len(reuters.words()))\n",
        "\n",
        "# And the distirbution of the words\n",
        "all_words = nltk.FreqDist(w.lower() for w in reuters.words())\n",
        "\n",
        "most_freq_words = all_words.most_common(2000)\n",
        "\n",
        "print('most freq words: ', most_freq_words[0:110])\n",
        "\n",
        "word_features = [word for (word, count) in most_freq_words]\n",
        "\n",
        "print('word_features[:25]: ', word_features[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za2TNvyhI_Bj",
        "outputId": "abf3a155-f85b-40d6-c46c-593db584fb3b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of words in the reuter corpus:  1720901\n",
            "most freq words:  [('.', 94687), (',', 72360), ('the', 69277), ('of', 36779), ('to', 36400), ('in', 29253), ('and', 25648), ('said', 25383), ('a', 25103), ('mln', 18623), ('s', 15680), ('vs', 14341), ('for', 13782), ('-', 13705), ('dlrs', 12417), (\"'\", 11272), ('it', 11104), ('000', 10277), ('1', 9977), ('pct', 9810), ('on', 9244), (';', 8762), ('&', 8698), ('lt', 8696), ('cts', 8361), ('from', 8217), ('is', 7668), ('that', 7540), ('year', 7529), ('>', 7449), ('its', 7402), ('by', 7101), ('at', 7017), ('net', 6989), ('\"', 6816), ('2', 6528), ('u', 6392), ('be', 6357), ('with', 6179), ('will', 5952), ('billion', 5829), ('was', 5816), ('he', 5215), ('loss', 5124), ('3', 5091), ('has', 4864), ('5', 4683), ('would', 4673), ('company', 4670), ('as', 4575), ('an', 4557), ('/', 4495), ('1986', 4392), ('not', 4389), ('4', 4363), ('shr', 4182), ('inc', 4121), ('which', 3666), ('bank', 3654), ('but', 3601), ('this', 3516), ('7', 3450), ('corp', 3399), (',\"', 3397), ('6', 3376), ('oil', 3272), ('or', 3248), ('last', 3243), ('8', 3218), ('are', 3215), ('share', 3160), ('have', 3107), ('trade', 3098), ('were', 3092), ('had', 2975), ('one', 2963), ('profit', 2960), ('0', 2928), ('9', 2864), ('(', 2840), ('about', 2814), ('market', 2811), ('new', 2716), ('qtr', 2674), ('two', 2665), ('shares', 2652), ('stock', 2629), ('they', 2595), ('also', 2532), ('tonnes', 2511), ('1987', 2396), ('10', 2381), ('revs', 2312), ('up', 2307), ('sales', 2217), ('prices', 2195), ('may', 2131), ('group', 2112), ('per', 2101), ('been', 2070), ('march', 2052), ('april', 2051), ('after', 2022), (')', 2005), ('first', 1919), ('co', 1897), ('japan', 1890), ('more', 1881), ('rate', 1875), ('quarter', 1852)]\n",
            "word_features[:25]:  ['.', ',', 'the', 'of', 'to', 'in', 'and', 'said', 'a', 'mln', 's', 'vs', 'for', '-', 'dlrs', \"'\", 'it', '000', '1', 'pct', 'on', ';', '&', 'lt', 'cts']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get the features from documents\n",
        "\n",
        "def get_document_features(document):\n",
        "    \"\"\"\n",
        "        This function will convert given document into a feature set.\n",
        "\n",
        "    \"\"\"\n",
        "    document_words = set(document)\n",
        "\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "documents = [(list(reuters.words(fileid)), category)\n",
        "    for category in reuters.categories()\n",
        "    for fileid in reuters.fileids(category)]\n"
      ],
      "metadata": {
        "id": "FvCtVQR3JPXw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(get_document_features(d), c) for (d,c) in documents]\n",
        "\n",
        "#you can experiment with different train/test splits\n",
        "train_set,test_set  = featuresets[1000:3000], featuresets[:1000]\n",
        "\n"
      ],
      "metadata": {
        "id": "YC74LIIOJY7l"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "# Instantiate the Naive Bayes Classifier\n",
        "\n",
        "\n",
        "\n",
        "# Check the accuracy\n",
        "\n",
        "\n",
        "\n",
        "# Check the most informative features\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CiI9hY9Qz_Rd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXGamtWpy2lH",
        "outputId": "e4f9d688-040d-4343-e5cd-0f8a933233bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.956\n",
            "Most Informative Features\n",
            "          contains(palm) = True           coconu : acq    =    627.9 : 1.0\n",
            "   contains(agriculture) = True           copra- : acq    =    570.8 : 1.0\n",
            "       contains(farmers) = True           copra- : acq    =    570.8 : 1.0\n",
            "         contains(wheat) = True           barley : acq    =    500.6 : 1.0\n",
            "        contains(tonnes) = True           copra- : acq    =    479.5 : 1.0\n",
            "        contains(quotas) = True           coffee : acq    =    407.7 : 1.0\n",
            "  contains(agricultural) = True           copra- : acq    =    342.5 : 1.0\n",
            "      contains(calendar) = True           copra- : acq    =    342.5 : 1.0\n",
            "   contains(commodities) = True           copra- : acq    =    342.5 : 1.0\n",
            "    contains(employment) = True           copra- : acq    =    342.5 : 1.0\n",
            "         contains(fresh) = True           copra- : acq    =    342.5 : 1.0\n",
            "        contains(modest) = True           copra- : acq    =    342.5 : 1.0\n",
            "        contains(rubber) = True           copra- : acq    =    342.5 : 1.0\n",
            "      contains(shortage) = True           copra- : acq    =    342.5 : 1.0\n",
            "          contains(oils) = True           coconu : acq    =    318.0 : 1.0\n",
            "        contains(copper) = True           copper : acq    =    283.7 : 1.0\n",
            "          contains(corn) = True             corn : acq    =    274.0 : 1.0\n",
            "        contains(cotton) = True           castor : acq    =    274.0 : 1.0\n",
            "       contains(figures) = True           coconu : acq    =    274.0 : 1.0\n",
            "       contains(freight) = True           castor : acq    =    274.0 : 1.0\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# SOLUTION\n",
        "\n",
        "# Instantiate the Naive Bayes Classifier\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Check the accuracy\n",
        "print(nltk.classify.accuracy(classifier, test_set))\n",
        "\n",
        "# Check the most informative features\n",
        "print(classifier.show_most_informative_features(20))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}