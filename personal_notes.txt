prec_at_k = []
recall_at_k = []
for i, label_features in enumerate(tokenized_labels_features):
	sim = (100.0 * label_features @ all_image_features.T).softmax(dim=-1) # similarities between query and all images
	topk_probs, topk_indices = sim.topk(K, dim=-1)
	topk_pred_labels_idxs = [dataset_labels_int[topk_indices.squeeze().item()]] if K==1 else [dataset_labels_int[idx] for idx in topk_indices.squeeze().cpu().numpy()] # K@1, 5, ...
	relevant_retrieved_images_for_label_i = topk_pred_labels_idxs.count(i)# count number of relevant images (i.e., images with the same label) in top-K retrieved images.
	prec_at_k.append(relevant_retrieved_images_for_label_i/K)
	all_images_with_label_i = [idx for idx, (img, lbl) in enumerate(zip(dataset_images_id, dataset_labels_int)) if lbl == i]
	num_all_images_with_label_i = len(all_images_with_label_i)
	recall_at_k.append(relevant_retrieved_images_for_label_i/num_all_images_with_label_i)
avg_prec_at_k = sum(prec_at_k)/len(labels)
avg_recall_at_k = sum(recall_at_k) / len(labels)
print(f"Precision@{K}: {avg_prec_at_k:.3f} {np.mean(prec_at_k)}")
print(f"Recall@{K}: {avg_recall_at_k} {np.mean(recall_at_k)}")
print(f"Elapsed_t: {time.time()-t0:.3f} sec".center(160, "-"))

############################################################################################################################
# Check for plateau to adapt phases of progressive freezing
# if epoch > 0 and len(validation_losses) > 1:
# 	current_smoothed_loss = smooth_(losses=validation_losses, window=3)
# 	smoothed_val_losses.append(current_smoothed_loss)
# 	if len(smoothed_val_losses) > 1:
# 		loss_diff = smoothed_val_losses[-2] - smoothed_val_losses[-1]
# 		if loss_diff < plateau_threshold:
# 			counter += 1
# 			print(f"Plateau counter: {counter}/{patience_per_phase} (Smoothed loss: {current_smoothed_loss:.6f})")
# 		else:
# 			counter = 0
# 			print(f"No plateau detected. Continuing current phase. (Smoothed loss: {current_smoothed_loss:.6f})")
# 		if counter >= patience_per_phase and current_phase < len(freeze_schedule) - 1:
# 			current_phase += 1
# 			counter = 0
# 			learning_rate = initial_learning_rate * (0.1 ** current_phase) # Reduce learning rate by 10x for each new phase
# 			print(f"Plateau detected. Transitioning to Phase {current_phase} with updated LR: {learning_rate:.1e}")
############################################################################################################################


>> Fine-tuning a pre-trained model using conventional backpropagation:
# logits_per_image: similarity scores between each image embedding and all text embeddings in the batch
# Each row in logits_per_image corresponds to one image in the batch, and each column corresponds to a text description.

# logits_per_text: similarity scores between each text embedding and all image embeddings in the batch

# # Conventional backpropagation:
# logits_per_image, logits_per_text = model(images, labels) # torch.Size([batch_size, batch_size]) torch.Size([batch_size, batch_size])
# ground_truth = torch.arange(start=0, end=len(images), dtype=torch.long, device=device)
# loss_img = criterion(logits_per_image, ground_truth) 
# loss_txt = criterion(logits_per_text, ground_truth)
# total_loss = 0.5 * (loss_img + loss_txt)
# total_loss.backward()
# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
# optimizer.step() # Update weights

# def evaluate(model, test_loader, criterion, device:str="cuda"):
# 	model.eval()
# 	total_loss = 0
# 	total_correct_text_description_for_each_image = 0
# 	total_correct_image_for_each_text_description = 0
# 	with torch.no_grad():
# 		for batch_idx, (images, labels) in enumerate(test_loader):
# 			images, labels = images.to(device), labels.to(device)
# 			logits_per_image, logits_per_text = model(images, labels) # torch.Size([batch_size, batch_size]) torch.Size([batch_size, batch_size])
# 			_, predicted_idxs_imgs = torch.max(input=logits_per_image, dim=1, keepdim=True)
# 			_, predicted_idxs_txts = torch.max(input=logits_per_text, dim=1, keepdim=True)
# 			correct_text_description_idxs = torch.argmax(labels, dim=1) # indices of correct text descriptions for each image
# 			# Compare predicted indexes with the correct indexes
# 			total_correct_text_description_for_each_image += (predicted_idxs_imgs == correct_text_description_idxs.unsqueeze(1)).sum().item()
# 			total_correct_image_for_each_text_description += (predicted_idxs_txts == correct_text_description_idxs.unsqueeze(1)).sum().item()

# 			# validation loss
# 			ground_truth = torch.arange(start=0, end=len(images), dtype=torch.long, device=device)
# 			loss_img = criterion(logits_per_image, ground_truth) 
# 			loss_txt = criterion(logits_per_text, ground_truth)
# 			valid_loss = 0.5 * (loss_img + loss_txt)
# 			total_loss += valid_loss.item()
# 	avg_loss = total_loss / len(test_loader)
# 	accuracy_text_description_for_each_image = total_correct_text_description_for_each_image / len(test_loader.dataset)
# 	accuracy_text_image_for_each_text_description = total_correct_image_for_each_text_description / len(test_loader.dataset)
# 	return avg_loss, accuracy_text_description_for_each_image, accuracy_text_image_for_each_text_description


more advanced:
def evaluate(model, test_loader, criterion, device="cuda"):
	model.eval()
	total_loss = 0
	correct_text_description = 0
	correct_image_for_text = 0
	total_samples = 0
	with torch.no_grad():
		for bidx, (images, labels) in enumerate(test_loader):
			images, labels = images.to(device), labels.to(device)
			batch_size = images.size(0)
			total_samples += batch_size
			logits_per_image, logits_per_text = model(images, labels) # torch.Size([batch_size, batch_size]) torch.Size([batch_size, batch_size])
			# Predictions
			predicted_text_idxs = torch.argmax(input=logits_per_image, dim=1) # indices of maximum value of all elements in input tensor. torch.Size([batch_size])
			predicted_image_idxs = torch.argmax(input=logits_per_text, dim=1)
			correct_labels = torch.arange(start=0, end=batch_size, dtype=torch.long, device=device) # ground truth labels for each batch item torch.Size([batch_size])
			# Metrics
			correct_text_description += (predicted_text_idxs == correct_labels).sum().item()
			correct_image_for_text += (predicted_image_idxs == correct_labels).sum().item()
			# Validation loss
			loss_img = criterion(logits_per_image, correct_labels)
			loss_txt = criterion(logits_per_text, correct_labels)
			total_loss += 0.5 * (loss_img.item() + loss_txt.item())
	# Compute average loss and accuracies
	avg_loss = total_loss / len(test_loader)
	accuracy_text_description = correct_text_description / total_samples
	accuracy_image_for_text = correct_image_for_text / total_samples
	return avg_loss, accuracy_text_description, accuracy_image_for_text


###################################################################################
# GPU cosine similarity + Average recommendation vector:
def get_customized_cosine_similarity_gpu(spMtx, query_vec, idf_vec, spMtx_norm, exponent:float=1.0, batch_size:int=2048):
		print(f"[GPU Optimized] Customized Cosine Similarity (1 x nUsers={spMtx.shape[0]}) batch_size={batch_size}".center(130, "-"))
		print(
			f"Query: {query_vec.shape} {type(query_vec)} {query_vec.dtype} non_zeros={np.count_nonzero(query_vec)} (ratio={np.count_nonzero(query_vec) / query_vec.size})\n"
			f"spMtx {type(spMtx)} {spMtx.shape} {spMtx.dtype}\n"
			f"spMtxNorm: {type(spMtx_norm)} {spMtx_norm.shape} {spMtx_norm.dtype}\n"
			f"IDF {type(idf_vec)} {idf_vec.shape} {idf_vec.dtype}"
		)
		# Clear memory before starting
		cp.get_default_memory_pool().free_all_blocks()
		torch.cuda.empty_cache()
		
		# Print GPU device information
		device = cp.cuda.Device()
		device_id = device.id
		device_name = cp.cuda.runtime.getDeviceProperties(device_id)['name'].decode('utf-8')
		print(f"GPU: {device_name} ({device_id})")
		print(f"Initial Free GPU Memory: {device.mem_info[0] / 1024 ** 3:.2f} GB / Total GPU Memory: {device.mem_info[1] / 1024 ** 3:.2f} GB")

		st_t = time.time()
		# Convert inputs to CuPy arrays (float32)
		query_vec_squeezed = cp.asarray(query_vec.ravel(), dtype=cp.float32)
		idf_squeezed = cp.asarray(idf_vec.ravel(), dtype=cp.float32)
		spMtx_norm = cp.asarray(spMtx_norm, dtype=cp.float32)

		# Convert sparse matrix to CuPy CSR format
		spMtx_csr = spMtx.tocsr()
		spMtx_gpu = cp.sparse.csr_matrix(
				(cp.asarray(spMtx_csr.data, dtype=cp.float32), cp.asarray(spMtx_csr.indices), cp.asarray(spMtx_csr.indptr)),
				shape=spMtx_csr.shape
		)

		# Compute quInterest and its norm
		quInterest = query_vec_squeezed * idf_squeezed
		quInterestNorm = cp.linalg.norm(quInterest)

		# Get indices of non-zero elements in quInterest
		idx_nonzeros = cp.nonzero(quInterest)[0]
		quInterest_nonZeros = quInterest[idx_nonzeros] / quInterestNorm

		# Normalize user interests
		usrInterestNorm = spMtx_norm + cp.float32(1e-4)

		# Initialize result array
		cs = cp.zeros(spMtx_gpu.shape[0], dtype=cp.float32)

		# Process in batches to avoid memory overflow
		for i in range(0, spMtx_gpu.shape[0], batch_size):
				# Define batch range
				start_idx = i
				end_idx = min(i + batch_size, spMtx_gpu.shape[0])

				# Extract batch from sparse matrix
				spMtx_batch = spMtx_gpu[start_idx:end_idx, :]

				# Extract only the necessary columns from the batch
				spMtx_nonZeros = spMtx_batch[:, idx_nonzeros]

				# Apply IDF and normalize
				spMtx_nonZeros = spMtx_nonZeros.multiply(idf_squeezed[idx_nonzeros])
				spMtx_nonZeros = spMtx_nonZeros.multiply(1 / usrInterestNorm[start_idx:end_idx, None])

				# Apply exponent if necessary
				if exponent != 1.0:
						spMtx_nonZeros.data **= exponent

				# Compute cosine similarity scores for the batch
				cs_batch = spMtx_nonZeros.dot(quInterest_nonZeros)

				# Store batch results
				cs[start_idx:end_idx] = cs_batch

				# Free memory for the batch
				del spMtx_batch, spMtx_nonZeros, cs_batch
				cp.get_default_memory_pool().free_all_blocks()
				torch.cuda.empty_cache() # Clear CUDA cache
				# torch.cuda.synchronize() # Ensure all CUDA operations are complete
				# Print memory usage after each batch
				# print(f"Batch {i // batch_size + 1}: Free GPU Memory: {device.mem_info[0] / 1024 ** 3:.2f} GB")

		print(f"Elapsed_t: {time.time() - st_t:.2f} s {type(cs)} {cs.dtype} {cs.shape}".center(130, " "))
		return cp.asnumpy(cs)  # Convert result back to NumPy for compatibility

def get_customized_recsys_avg_vec_gpu(spMtx, cosine_sim, idf_vec, spMtx_norm, batch_size:int=2048):
		print(f"[GPU optimized] avgRecSys (1 x nTKs={spMtx.shape[1]})".center(130, "-"))
		st_t = time.time()
		
		# Move data to GPU
		idf_squeezed = cp.asarray(idf_vec.ravel(), dtype=cp.float32)
		cosine_sim_gpu = cp.asarray(cosine_sim, dtype=cp.float32)
		spMtx_norm_gpu = cp.asarray(spMtx_norm, dtype=cp.float32)
		
		# Find non-zero cosine similarities
		non_zero_cosines = cp.nonzero(cosine_sim_gpu)[0]
		non_zero_values = cosine_sim_gpu[non_zero_cosines]
		
		print(
				f"spMtx {type(spMtx)} {spMtx.shape} {spMtx.dtype}\n"
				f"spMtxNorm: {type(spMtx_norm)} {spMtx_norm.shape} {spMtx_norm.dtype}\n"
				f"CS {type(cosine_sim)} {cosine_sim.shape} {cosine_sim.dtype} NonZero(s): {len(non_zero_cosines)}\n"
				f"IDF {type(idf_vec)} {idf_vec.shape} {idf_vec.dtype}"
		)
		
		# Convert sparse matrix to CuPy CSR format
		spMtx_csr = spMtx.tocsr()
		spMtx_gpu = cp.sparse.csr_matrix(
				(cp.asarray(spMtx_csr.data, dtype=cp.float32),
				 cp.asarray(spMtx_csr.indices),
				 cp.asarray(spMtx_csr.indptr)),
				shape=spMtx_csr.shape
		)
		
		# Initialize result array on GPU
		avg_rec = cp.zeros(spMtx.shape[1], dtype=cp.float32)
		
		# Process in batches
		for i in range(0, len(non_zero_cosines), batch_size):
				batch_indices = non_zero_cosines[i:i + batch_size]
				batch_values = non_zero_values[i:i + batch_size]
				
				# Extract batch from sparse matrix
				spMtx_batch = spMtx_gpu[batch_indices]
				
				# Apply IDF
				batch_result = spMtx_batch.multiply(idf_squeezed)
				
				# Normalize by user interest norm
				norm_factors = spMtx_norm_gpu[batch_indices] + cp.float32(1e-18)
				batch_result = batch_result.multiply(1.0 / norm_factors[:, None])
				
				# Multiply by cosine similarities
				batch_result = batch_result.multiply(batch_values[:, None])
				
				# Add to running sum
				avg_rec += batch_result.sum(axis=0).ravel()
				
				# Clean up memory
				del batch_result, spMtx_batch
				cp.get_default_memory_pool().free_all_blocks()
		
		# Normalize the result
		avg_rec /= cp.sum(non_zero_values)
		
		# Convert back to CPU
		result = cp.asnumpy(avg_rec)
		
		print(f"Elapsed_t: {time.time()-st_t:.2f} s {type(result)} {result.dtype} {result.shape}".center(130, " "))
		return result

if USER == "farid":
for q in ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']:
	get_text_to_images(
		dataset=valid_dataset,
		model=model,
		# preprocess=preprocess,
		query=q,
		topk=args.topK,
		batch_size=args.batch_size,
	)
