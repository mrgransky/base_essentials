0. Connect to Pouta server:
$ ssh -vi $HOME/.ssh/nlf_key ubuntu@128.214.254.157
$ ssh ubuntu@128.214.254.157 # should work with no need to keys
$ nslookup 128.214.254.157
$ lsof -i :8000

# to access your Django app from VM
$ ssh -L 8000:localhost:8000 ubuntu@128.214.254.157

$ rm -rfv ~/stanza_resources/ # once in while or after updating stanza
# only for Local Laptop
$ rm -rf staticfiles; python manage.py collectstatic; clear; python manage.py runserver

# in Pouta:
$ rm -rf staticfiles; python manage.py collectstatic; 

# RecSys;
$ python manage.py runserver 0.0.0.0:8000

# ImACCESS:
$ python manage.py runserver 0.0.0.0:9000

1. Connect to SLURM account from Local machine:
	$ ssh -vi $HOME/.ssh/narvi_key alijani@narvi.tut.fi # my new precision DELL laptop
	$ ssh -vi $HOME/narvi_key/xenial-narvi-key alijani@narvi.tut.fi
	$ ssh alijanif@puhti.csc.fi
	$ ssh alijanif@lumi.csc.fi

	# LUMI activate python
	$ module load cray-python

	# module list 	List currently loaded modules.
	# module avail 	List available packages.
	# module spider 	List available packages in a different format.
	# module help [modulefile] 	Description of specified module.
	# module show [modulefile] 	Displays information about specified module, including environment changes, dependencies, software version and path.
	# module load [modulefile] 	Loads module or specifies which dependencies have not been loaded.
	# module unload [modulefile] 	Unloads specified module from environment.
	# module purge 	Unloads all loaded modules

	# info about OS:
	$ cat /etc/os-release

2. mount working_folders from narvi computer to local: (type in local machine)
	$ sshfs -o IdentityFile=/home/xenial/narvi_key/xenial-narvi-key alijani@narvi.tut.fi:example_folder /home/xenial/narvi_ws/
	
3. To unmount files from this machine:
	$ fusermount -u /home/xenial/narvi_ws

4. jupyter notebook:
	# convert ipynb to python script:
	$ jupyter nbconvert mynotebook.ipynb --to python

	#1) remote machine: i.e., Narvi, Puhti
		$ jupyter notebook --no-browser --port=8787
	
	#2) in a new terminal on local machine
		#$ ssh -i $HOME/narvi_key/xenial-narvi-key alijani@narvi.tut.fi -N -f -L localhost:8888:localhost:8787 alijani@narvi.tut.fi
		$ ssh -i $HOME/narvi_key/xenial-narvi-key alijani@narvi.tut.fi -N -f -L -v -v 8787:127.0.0.1:8787 alijani@narvi.tut.fi
		#$ ssh -i $HOME/narvi_key/xenial-narvi-key alijani@narvi.tut.fi -N -f -L 8889:[::1]:8889 alijani@narvi.tut.fi
		
		# puhti:
		#$ ssh -f alijanif@puhti.csc.fi -L 8382:127.0.0.1:7575 -N
		$ ssh -f alijanif@puhti.csc.fi -L 8889:[::1]:8889 -N
	
	open firefox:
		token pass from remote terminal 
		
5. copy from local -> server:
	# Narvi:
	$ rsync -avzh --stats --progress -e "ssh -i $HOME/narvi_key/xenial-narvi-key" /home/xenial/Datasets/Nationalbiblioteket/NLF_Pseudonymized_Logs alijani@narvi.tut.fi:/lustre/sgn-data/vision/
	
	# Puhti:
	$ rsync -avzhm --stats --progress $HOME/Datasets/Oxford_RobotCar/rtk/2015-11-12-11-22-05/rtk.csv alijanif@puhti.csc.fi:/scratch/project_2004072/OxfordRobotCar/2015-11-12-11-22-05/gps/
	$ rsync -avzhm --stats --progress $HOME/Datasets/Nationalbiblioteket/*.zip alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket
	$ rsync -azhm --stats --progress $HOME/datasets/MS_COCO alijanif@puhti.csc.fi:/scratch/project_2004072/IMG_Captioning &> /dev/null # no prininting in terminal

	# local laptop => Puhti: [must be executed in local laptop terminal]:
	$ rsync -avzhm --stats --progress $HOME/datasets/Nationalbiblioteket/dataframes_x732/*.json alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/dataframes_x732 -vv

	# Pouta => Puhti: (Execute in Pouta terminal)
	$ rsync -avzhm --stats --progress /media/volume/Nationalbiblioteket/dataframes_xx/nike* alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/dataframes_x732/ -vv
	$ rsync -avzhm --stats --progress /media/volume/Nationalbiblioteket/dataframes_xx/nike6_docworks_lib_helsinki_fi_access_log_2021-11-*pre* alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/dataframes_x732/ -vv
	
	# local laptop => Pouta
	$ rsync -avzh --stats --progress $HOME/datasets/compressed_concatenated_SPMs/*.tar.gz ubuntu@128.214.254.157:datasets/compressed_concatenated_SPMs
	$ rsync -avzh --stats --progress $HOME/datasets/sentinel2-l1c* ubuntu@128.214.254.157:/media/volume/datasets	
	$ rsync -avzh --stats --progress $HOME/datasets/Nationalbiblioteket ubuntu@128.214.254.157:/media/volume/datasets	
	
	# openpose caffemodels:
	$ rsync -avzh --stats --progress $HOME/WS_Farid/ImACCESS/openpose/models ubuntu@128.214.254.157:WS_Farid/ImACCESS/openpose
	# captioning models:
	$ rsync -avzh --stats --progress $HOME/datasets/trash/models ubuntu@128.214.254.157:/media/volume/ImACCESS
	
	# SERVER => SERVER is NOT possible:
	# The source and destination cannot both be remote.
	# rsync error: syntax or usage error (code 1) at main.c(1428) [Receiver=3.2.7]
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/compressed_concatenated_SPMs/*114*.tar.gz ubuntu@128.214.254.157:datasets/compressed_concatenated_SPMs -vv

	# do the following instead (for all):
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/compressed_concatenated_SPMs/*.tar.gz $HOME/datasets/compressed_concatenated_SPMs/ -vv

	# local laptop [MUST be executed in local laptop terminal]:
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/compressed_concatenated_SPMs/*.tar.gz $HOME/datasets/compressed_concatenated_SPMs/ -vv
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/dataframes_x732/concat*.json $HOME/datasets/Nationalbiblioteket/dataframes_x732/ -vv

	# to local laptop (from LUMI) (must be executed in local laptop):
	$ rsync -avzh --stats --progress alijanif@lumi.csc.fi:WS_Farid/ProgrammingModels $HOME/MISC -vv
	$ rsync -avzh --stats --progress ubuntu@128.214.254.157:~/WS_Farid/ImACCESS/openpose/examples/tutorial_api_python/outputs $HOME/datasets/trash/

	$ rsync -avzh --stats --progress ubuntu@128.214.254.157:~/WS_Farid/ImACCESS/openpose/outputs $HOME/datasets/trash/

	# To Pouta (must be done in Pouta) for only one concat:
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/compressed_concatenated_SPMs/concat_x732* /media/volume/Nationalbiblioteket/compressed_concatenated_SPMs/ -vv
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/dataframes_x732/concat* /media/volume/Nationalbiblioteket/compressed_concatenated_SPMs/concat_x732_lm_stanza/ -vv

6. copy from server -> local:
	# To local laptop (must be executed locally):
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/compressed_concatenated_SPMs/concat_x2*.tar.gz $HOME/datasets/compressed_concatenated_SPMs/ -vv
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/compressed_concatenated_SPMs/concat_x* $HOME/datasets/compressed_concatenated_SPMs/ -vv
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/NLF_DATASET/nike* $HOME/datasets/Nationalbiblioteket/NLF_DATASET/ -vv
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/IMG_Captioning/trash/models/*.pkl $HOME/datasets/trash/models/ -vv
	
	# To Pouta (must be executed in Pouta): (newly scraped NLF dataset [17.04.2024]):
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/NLF_DATASET /media/volume/Nationalbiblioteket -vv

	$ rsync -avzh --stats --progress -e "ssh -i $HOME/narvi_key/xenial-narvi-key" alijani@narvi.tut.fi:/lustre/sgn-data/Nationalbiblioteket/dataframes/*.json $HOME/datasets/Nationalbiblioteket/trash -vv

	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/trash_log/retrievalSfM120k-vgg16-gem_th{5.0,10.0,15.0,20.0}* $HOME/WS_Farid/OxfordRobotCar/results/RC
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/preprocessed_dataframes_XX/nike*xlsx $HOME/datasets/Nationalbiblioteket/trash/docs -vv
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/datasets/nikeY.docworks.lib.helsinki.fi_access_log.07_02_2021.log.dump $HOME/datasets/Nationalbiblioteket/datasets

	$ rsync -avzh --stats --progress -e "ssh -i $HOME/narvi_key/xenial-narvi-key" alijani@narvi.tut.fi:/lustre/sgn-data/vision/OxfordRobotCar/my_imgs /home/xenial/WS_Farid/OxfordRobotCar/results/RC
	$ rsync -avzh --stats --progress -e "ssh -i $HOME/narvi_key/xenial-narvi-key" alijani@narvi.tut.fi:/lustre/sgn-data/vision/trash_log/retrievalSfM120k-vgg16-gem_th25.0m_AllQs /home/xenial/WS_Farid/OxfordRobotCar/results/RC
	$ rsync -avzh --stats --progress -e "ssh -i $HOME/narvi_key/xenial-narvi-key" alijani@narvi.tut.fi:/lustre/sgn-data/vision/trash_log/robotcar-oxford-train_mono_left_undistorted_velodyne_left_ranges_resnet50_gem_whiten_ep50_contrastive_m0.5_adam_lr1.0e-06_wd1.0e-06_3negs_qsz10_psz200_bsz5_img1024_th5.0m_Que123/plots* $HOME/WS_Farid/OxfordRobotCar/results/RC
	$ rsync -avzh --stats --progress -e "ssh -i $HOME/narvi_key/xenial-narvi-key" alijani@narvi.tut.fi:/lustre/sgn-data/vision/trash_log/cold_results/cold-train_std_cam_resnet50_pooling_spoc_whiten_ep50_contrastive_m0.5_adam_lr1.0e-06_wd1.0e-06_3negs_qsz10_psz200_bsz5_img640_th0.25m_Que123/plots* $HOME/WS_Farid/OxfordRobotCar/results/COLD
	$ rsync -avzh --stats --progress -e "ssh -i $HOME/narvi_key/xenial-narvi-key" alijani@narvi.tut.fi:/lustre/sgn-data/vision/trash_log/cold_results $HOME/WS_Farid/OxfordRobotCar/results/COLD

	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/trash_log/robotcar-oxford-train_velodyne_left_ranges_resnet50_gem_whiten_ep50_contrastive_m0.5_adam_lr1.0e-06_wd1.0e-06_3negs_qsz10_psz200_bsz5_img1024_th5.0m_Que123/plots* $HOME/WS_Farid/OxfordRobotCar/results/RC
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/trash_log/retrieval_train-resnet50-gem_th* /home/xenial/WS_Farid/OxfordRobotCar/results/RC
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:WS_Farid/OxfordRobotCar/VPR_IR/Tietotalo_log/floor1_double_check_r01g06_7339157.out /home/xenial/WS_Farid/OxfordRobotCar/VPR_IR/Tietotalo_log
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/dataframes_x*/*.tar.gz /home/farid/datasets/compressed_concatenated_SPMs

	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/GANs/misc_sngan*/{metrics,fake_IMGs,real_IMGs} /home/farid/datasets/GANs_results/misc_sngan
	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/GANs/misc_* --include="metrics" --include="fake_IMGs" --include="real_IMGs" --exclude="*" ~/datasets/GANs_results/ -vv

	$ rsync -avzh --stats --progress alijanif@puhti.csc.fi:/scratch/project_2004072/Nationalbiblioteket/dataframes_xx60/concatinated_60_SPMs_shrinked_*.json $PWD

########################## CONDA ##########################
7. package handling:
	#update conda:
	$ conda update -n base -c defaults conda
	
	$ conda info
	$ conda create -n py39 python=3.9 -y
	$ conda remove -n py39 --all -y
	$ conda remove -n myenv scipy

	$ conda activate py39
	$ pip install -r pip_narvi_requirements.txt # to install ready made packages in txt file (CBIR)
	
	$ conda env list 
	# OR
	$ conda info --envs
	# update all packages in a virtual env 
	$ conda update --all

	$ source $HOME/WS_Farid/essentials/gpu_intrc.sh
	
	$ conda config --add channels conda-forge # prioritize conda-forge
	$ conda config --remove channels conda-forge
	
	$ conda config --show 			# show all config
	$ conda config --show-sources

	$ cat $HOME/.condarc #  1) conda-forge , 2) defaults
	# or...
	$ conda config --show channels

	$ conda search pytorch -v
	$ #conda install pytorch torchvision cudatoolkit -c anaconda
	$ 
	$ conda remove scipy curl # 2 packages from current environment
	
	# all installed packages in my conda env
	$ conda list --show-channel-urls 

	$ python -c "import caffe; caffe.set_mode_gpu(); caffe.set_device(0)"
	
	$ python -c "import torch; import tensorflow as tf; print(tf.__version__); print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.current_device())"

	$ python -c "import cv2; print(cv2.__version__)"
	
	source /projappl/project_2004072/miniconda3/etc/profile.d/conda.sh

############################ Puhti Conda ###########################
$ module purge
$ module load tykky
$ cd /projappl/project_2004072
# modify packages.sh file as needed! <<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>
$ conda-containerize update CondaCSC --post-install packages.sh
$ export PATH="/projappl/project_2004072/CondaCSC/bin:$PATH"

############################ LUMI container wrapper ###########################
# similar to Puhti conda container wrapper:
# follow the doc: https://docs.lumi-supercomputer.eu/software/installing/container-wrapper/#lumi-container-wrapper
########################## SLURM ##########################

#how to see which job I am running:
	$ watch squeue -u alijani # monitor every 2 sec
	$ squeue --me
#run interactive cpu/gpu: to debug and run something...
	$ source interactive_cpu.sh
# OR... 
	$ source interactive_gpu.sh 

#run batch mode cpu/gpu:
	$ sbatch batch_cpu.sh
	$ sbatch --test-only puhti_sbatch.sh # find out when your job is estimated to run
	# sbatch: Job 13902002 to start at 2022-10-27T11:41:31 using 10 processors on nodes r03c21 in partition longrun

# OR... 
	$ sbatch batch_gpu.sh
#cancel 1 job:
	$ scancel job_id
#cancel all jobs:
	$ scancel -u alijani

# sinfo valid type specs, https://slurm.schedmd.com/sinfo.html
$ sinfo -o "%15P %3c %10m %11l %30G %14e %5D %25f"
$ sudo apt update -y && sudo apt upgrade -y

# This command will print the following information for each partition:
# 	%15P: Partition name
# 	%3c: Current number of jobs running in the partition
# 	%10m: Maximum number of jobs that can run in the partition simultaneously
# 	%11l: Number of nodes in the partition
# 	%30G: Total amount of memory in the partition
# 	%14e: Amount of memory currently in use in the partition
# 	%5D: Number of nodes currently available in the partition
# 	%25f: Current fair share of the partition

$ sacctmgr list user $USER withassoc

# 
$ squeue -u alijanif --long

# Analyzing currently running jobs with sstat:
$ sstat --jobs=your_job-id

# Show Job History and Accounting:
$ sacct -j <your_job_number> -o jobid,Partition,start,end,state,CPUTime,AllocCPUS,ExitCode
$ sacct -u alijanif

# look at a specific job in more detail, whether your job efficiently used the allocated resources.
$ seff <Jobid>

# view more verbose information about your job, by using the job id and running the following command:
#Note: This only works for jobs that are in currently in queue:
$ scontrol show job <your_job_number>
$ scontrol show partition <partition_name>
$ scontrol show <ENTITY>
# from --help:
# <ENTITY> may be "aliases", "assoc_mgr", "bbstat", "burstBuffer", 
# "config", "daemons", "dwstat", "federation", "frontend",  
# "hostlist", "hostlistsorted", "hostnames", "job",           
# "licenses", "node", "partition", "reservation", "slurmd", 
# "step", or "topology"

# GPU watch
$ watch nvidia-smi --format=csv --query-gpu=memory.total,utilization.gpu,memory.used,memory.free
$ nvidia-smi dmon -d 3 -s pcvumt
$ nvidia-smi --loop=1
# installation required:
$ pip install --upgrade nvitop gpustat stanza pip
$ sudo apt install nvtop
$ nvtop -m # requires conda activate py39
$ gpustat -cp --watch
$ nvtop


$ find Nordland/results/GoogleNet/*.csv -mtime -3 -print
$ find Nordland/results/VGG16/*.csv -mtime -3 -exec rm {} +
########################## SLURM ##########################

########################## VSCODE ##########################
cttl + K + 0: fold all codes
cttl + k + j: unfold all codes

cttl + PgUp/ PgDn: switch between terminals/tabs
########################## VSCODE ##########################

##################################################
# Counting Files in the Current Directory
$ ls -l /path_to_dir | wc -l
$ ls -U | grep -c '\.jpg$'

# avoid __pycache__ # DON NOT DELETE THIS LINE!!!!
sys.dont_write_bytecode = True 
##################################################

############################## Github ##############################
# Make Git store the username and password and it will never ask for them:
$ git config --global credential.helper store

export PYTHONDONTWRITEBYTECODE=1

# go back in git commit (2 steps back):
$ git reset HEAD~2
# chnage some files and continue new commits

# Github username:
mrgransky

# Github token, generated August 25 2021
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 # check for expiration

# Github token, generated Jan 17 2024
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 

# Github token, generated Jan 19 2024:
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

# store username & password for a session (cache it)
$ git config --global credential.helper cache

# store username & password permanent
$ git config credential.helper store

# to remove commit:
$ git log
$ git reset --hard HEAD~1 # once at the time to go back!

# Exclude folder or file to be transfered to git repository:
# in git project directory:
$ code .gitignore
# copy files or folder names u don't want to push to git repo
# example copy these into .gitignore file u created!:
results
__pycache__
.gitignore

# git pull from branch "farid":
$ git checkout farid # not required every single time!
$ git pull origin farid -v
############################## Github ##############################

# How to download Oxford Radar RobotCar Dataset with Command line:
$ python download.py --datasets="2019-01-14-14-15-12-radar-oxford-10k,2019-01-15-14-24-38-radar-oxford-10k,2019-01-18-14-14-42-radar-oxford-10k" --download_folder /lustre/sgn-data/vision/OxfordRadarRobotCar --sensors="Grasshopper 2 Right,Velodyne HDL-32E Left Pointcloud,Velodyne HDL-32E Right,Bumblebee XB3 Visual Odometry,Navtech CTS350-X Radar"
$ python download.py --datasets="2019-01-14-14-15-12-radar-oxford-10k,2019-01-15-14-24-38-radar-oxford-10k,2019-01-18-14-14-42-radar-oxford-10k" --download_folder /lustre/sgn-data/vision/OxfordRadarRobotCar --sensors="Velodyne HDL-32E Left Pointcloud,Velodyne HDL-32E Right Pointcloud"

# puhti
$ python download.py --datasets="2019-01-14-14-15-12-radar-oxford-10k,2019-01-15-14-24-38-radar-oxford-10k,2019-01-18-14-14-42-radar-oxford-10k" --download_folder /scratch/project_2004072/OxfordRadarRobotCar --sensors="Grasshopper 2 Left,Velodyne HDL-32E Left,Bumblebee XB3 Visual Odometry,NovAtel GPS / INS"
$ python download.py --datasets="2019-01-14-14-15-12-radar-oxford-10k,2019-01-15-14-24-38-radar-oxford-10k,2019-01-18-14-14-42-radar-oxford-10k" --download_folder $HOME/Datasets/Oxford_Radar_RobotCar --sensors="Grasshopper 2 Left,Velodyne HDL-32E Left,Bumblebee XB3 Visual Odometry,NovAtel GPS / INS"

############################## Pytorch CUDA info ##############################
>>> import torch

>>> torch.cuda.is_available()
True

>>> torch.cuda.device_count()
1

>>> torch.cuda.current_device()
0

>>> torch.cuda.device(0)
<torch.cuda.device at 0x7efce0b03be0>

>>> torch.cuda.get_device_name(0)
'GeForce GTX 950M'

# in python script:
import torch
torch.cuda.init()
print(torch.randn(1, device='cuda'))

##################################################################################################
# nohup:
$ nohup python -u RecSys_usr_token.py --inputDF /scratch/project_2004072/Nationalbiblioteket/dataframes/nikeY.docworks.lib.helsinki.fi_access_log.07_02_2021.log.dump --qphrase 'åbo akademi' --lmMethod 'stanza' > nikeY_stanza.out &
$ nohup python -u manage.py --inputDF /scratch/project_2004072/Nationalbiblioteket/dataframes/nikeY.docworks.lib.helsinki.fi_access_log.07_02_2021.log.dump --qphrase 'åbo akademi' --lmMethod 'stanza' > nikeY_stanza.out &

# kill nohup in terminal:
$ jobs -l # get pid of the running jobs

$ kill PID_NUMBER

# Ubuntu:

# delete file delete list files from nikeQ_30_xxx.txt to nikeQ_59_xxx.txt
$ rm -rfv /scratch/project_2004072/Nationalbiblioteket/trash/NLF_logs/nikeQ_{30..60}_*.out

# To delete all files in a directory before a certain date (older than N days), using find command in combination with rm.
# check with ls first:
$ find /media/volume/Nationalbiblioteket/dataframes_x19 -type f ! -newermt '2024-04-22 10:00:00' -exec ls -lt {} \; 

# delete:
$ find /media/volume/Nationalbiblioteket/trash/*nk_q_* -type f ! -newermt '2024-04-22 11:00:00' -exec rm -rfv {} \; # for same day delete
$ find /media/volume/Nationalbiblioteket/dataframes_x19 -type f -mtime +N -exec rm {} \;

# symbolic link: use ln together with special pattern matching characters
# links all the files with the extension ".xdh" in sub-directory project to current directory
$ ln -s project/*.xdh $PWD

# To check whether a package is installed or not:
$ dpkg -l | less # walk through all installed packages
$ dpkg -l {package_name}
$ dpkg -l vlc # example 

# tar multiple files: #TODO: python code at concat_dfs.py at the end!!!
$ cd /path/to/dir
$ tar -cvzf my_files.tar.gz file1 file2

# copy whole folder from google drive:
# find the folder id by navigating to the folder and copy & paste it:
$ gdown --folder 1rstAr9W4PC2ueHyLH-Igoxifrzv7aD2Z

# everytime something changes...
# pod name:
$ oc get pods | grep Running
$ oc rsync /scratch/project_2004072/Nationalbiblioteket/compressed_concatenated_SPMs recsys-44-tzf5s:/opt/app-root/src/datasets

####################################################### OpenShift Container Platform ########################################################
$ oc login https://rahti.csc.fi:8443 --token=changes_all_the_time_check
$ oc get projects

# check DeploymentConfig: 
# image: docker-registry.default.svc:5000/nlf/recsys@sha256:3e22e58a80e635b90d77845a5ccc14c8ed5ff616360f2d2a48ff12fb86545a19
$ oc describe DeploymentConfig recsys

# get pods name:
$ oc get pods | grep "Running" # get the running pods name

# copy to rahti: [Pods name must be replaced by above commands retult]
$ oc rsync /scratch/project_2004072/Nationalbiblioteket/compressed_concatenated_SPMs recsys-124-pphf6:/opt/app-root/src/datasets

$ oc describe quota
$ oc describe limitranges

# Prompting:
# check carefully for correctness, style and efficiency, and give constructive critisism for how to improve it.