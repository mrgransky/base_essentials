def get_text_to_image_linear_probe_accuracy(
	train_dataset,
	val_dataset,
	model,
	preprocess,
	device: str = "cuda:0",
	batch_size: int = 64,
	seed: int = 42
	):
	"""
	Compute Linear Probe Accuracy for Text-to-Image retrieval.
	:param train_dataset: Training dataset.
	:param val_dataset: Validation dataset.
	:param model: CLIP model.
	:param preprocess: Preprocessing for images.
	:param device: Device to run computations on.
	:param batch_size: Batch size for processing images.
	:param seed: Random seed for reproducibility.
	:return: Linear Probe Accuracy.
	"""
	print(f"Text-to-Image Linear Probe Accuracy".center(160, " "))
	
	# Extract text features from labels
	def get_text_features(dataset):
			labels = sorted(list(set(dataset["label"].tolist())))
			text_inputs = clip.tokenize(labels).to(device)
			with torch.no_grad():
					text_features = model.encode_text(text_inputs)
			text_features /= text_features.norm(dim=-1, keepdim=True)
			return text_features.cpu().numpy(), labels
	
	train_features, train_labels = get_text_features(train_dataset)
	val_features, val_labels = get_text_features(val_dataset)
	print(f"Training features[{type(train_features)}]: {train_features.shape}")
	print(f"Validation features[{type(val_features)}]: {val_features.shape}")
	# Label mappings
	label_dict = {lbl: idx for idx, lbl in enumerate(train_labels)}
	train_labels_int = [label_dict[lbl] for lbl in train_dataset["label"].tolist()]
	val_labels_int = [label_dict[lbl] for lbl in val_dataset["label"].tolist()]
	print(f"Training labels[{type(train_labels_int)}]: {len(train_labels_int)}")
	print(f"Validation labels[{type(val_labels_int)}]: {len(val_labels_int)}")
	# Train logistic regression
	classifier = LogisticRegression(
			random_state=seed,
			C=0.316,
			max_iter=1000,
			tol=1e-4,
			verbose=1,
			solver='saga',
			n_jobs=-1
	)
	classifier.fit(train_features, train_labels_int)
	
	# Evaluate
	val_features = clip.tokenize(val_labels).to(device)
	with torch.no_grad():
			val_features = model.encode_text(val_features).cpu().numpy()
	predictions = classifier.predict(val_features)
	
	return np.mean(predictions == val_labels_int)

def get_text_to_image_zero_shot_accuracy(
		dataset,
		model,
		preprocess,
		K: int = 5,
		device: str = "cuda:0",
		batch_size: int = 64,
		image_features_file: str = "validation_image_features.gz"
	):
		"""
		Compute Zero Shot Accuracy for Text-to-Image retrieval.
		:param dataset: Validation dataset with image paths and labels.
		:param model: CLIP model.
		:param preprocess: Preprocessing for images.
		:param K: Number of top predictions to consider.
		:param device: Device to run computations on.
		:param batch_size: Batch size for processing images.
		:param image_features_file: Path to precomputed image features.
		:return: Zero Shot Accuracy.
		"""
		print(f"Text-to-Image Zero Shot Accuracy (K={K})".center(160, " "))
		# Create label-to-integer mapping
		label_dict = {label: label_int for label, label_int in zip(dataset["label"], dataset["label_int"])}
		# Load or compute image features
		if not os.path.exists(image_features_file):
				image_features = []
				for i in range(0, len(dataset["img_path"]), batch_size):
						batch_paths = dataset["img_path"][i:i+batch_size]
						batch_tensors = torch.stack([preprocess(Image.open(path)).to(device) for path in batch_paths])
						with torch.no_grad():
								batch_features = model.encode_image(batch_tensors)
								batch_features /= batch_features.norm(dim=-1, keepdim=True)
						image_features.extend(batch_features.cpu().numpy())
				image_features = np.array(image_features)
				np.save(image_features_file, image_features)
		else:
				image_features = np.load(image_features_file)
		
		# Get unique labels to use as text queries
		labels = sorted(list(set(dataset["label"].tolist())))
		text_inputs = clip.tokenize(labels).to(device)
		
		# Compute text features for these labels
		with torch.no_grad():
				text_features = model.encode_text(text_inputs)
		text_features /= text_features.norm(dim=-1, keepdim=True)
		text_features = text_features.cpu().numpy()
		
		# Normalize image features
		image_features = image_features / np.linalg.norm(image_features, axis=1, keepdims=True)
		
		# Compute similarities and retrieve top-K images
		similarities = text_features @ image_features.T  # (num_labels, num_images)
		top_k_indices = np.argsort(-similarities, axis=-1)[:, :K]
		
		# Calculate accuracy: Check if any of the top-K images match the ground-truth label
		ground_truth = np.array(dataset["label_int"].tolist())
		accuracies = []
		for label_idx, label in enumerate(labels):
				true_indices = np.where(ground_truth == label_dict[label])[0]
				retrieved_indices = top_k_indices[label_idx]
				count = len(set(retrieved_indices) & set(true_indices))
				accuracies.append(count > 0)
		zero_shot_accuracy = np.mean(accuracies)
		print(f"Top-{K} Zero-Shot Accuracy: {zero_shot_accuracy:.3f}")
		return zero_shot_accuracy


prec_at_k = []
recall_at_k = []
for i, label_features in enumerate(tokenized_labels_features):
	sim = (100.0 * label_features @ all_image_features.T).softmax(dim=-1) # similarities between query and all images
	topk_probs, topk_indices = sim.topk(K, dim=-1)
	topk_pred_labels_idxs = [dataset_labels_int[topk_indices.squeeze().item()]] if K==1 else [dataset_labels_int[idx] for idx in topk_indices.squeeze().cpu().numpy()] # K@1, 5, ...
	relevant_retrieved_images_for_label_i = topk_pred_labels_idxs.count(i)# count number of relevant images (i.e., images with the same label) in top-K retrieved images.
	prec_at_k.append(relevant_retrieved_images_for_label_i/K)
	all_images_with_label_i = [idx for idx, (img, lbl) in enumerate(zip(dataset_images_id, dataset_labels_int)) if lbl == i]
	num_all_images_with_label_i = len(all_images_with_label_i)
	recall_at_k.append(relevant_retrieved_images_for_label_i/num_all_images_with_label_i)
avg_prec_at_k = sum(prec_at_k)/len(labels)
avg_recall_at_k = sum(recall_at_k) / len(labels)
print(f"Precision@{K}: {avg_prec_at_k:.3f} {np.mean(prec_at_k)}")
print(f"Recall@{K}: {avg_recall_at_k} {np.mean(recall_at_k)}")
print(f"Elapsed_t: {time.time()-t0:.3f} sec".center(160, "-"))

############################################################################################################################
# Check for plateau to adapt phases of progressive freezing
# if epoch > 0 and len(validation_losses) > 1:
# 	current_smoothed_loss = smooth_(losses=validation_losses, window=3)
# 	smoothed_val_losses.append(current_smoothed_loss)
# 	if len(smoothed_val_losses) > 1:
# 		loss_diff = smoothed_val_losses[-2] - smoothed_val_losses[-1]
# 		if loss_diff < plateau_threshold:
# 			counter += 1
# 			print(f"Plateau counter: {counter}/{patience_per_phase} (Smoothed loss: {current_smoothed_loss:.6f})")
# 		else:
# 			counter = 0
# 			print(f"No plateau detected. Continuing current phase. (Smoothed loss: {current_smoothed_loss:.6f})")
# 		if counter >= patience_per_phase and current_phase < len(freeze_schedule) - 1:
# 			current_phase += 1
# 			counter = 0
# 			learning_rate = initial_learning_rate * (0.1 ** current_phase) # Reduce learning rate by 10x for each new phase
# 			print(f"Plateau detected. Transitioning to Phase {current_phase} with updated LR: {learning_rate:.1e}")
############################################################################################################################


>> Fine-tuning a pre-trained model using conventional backpropagation:
# logits_per_image: similarity scores between each image embedding and all text embeddings in the batch
# Each row in logits_per_image corresponds to one image in the batch, and each column corresponds to a text description.

# logits_per_text: similarity scores between each text embedding and all image embeddings in the batch

# # Conventional backpropagation:
# logits_per_image, logits_per_text = model(images, labels) # torch.Size([batch_size, batch_size]) torch.Size([batch_size, batch_size])
# ground_truth = torch.arange(start=0, end=len(images), dtype=torch.long, device=device)
# loss_img = criterion(logits_per_image, ground_truth) 
# loss_txt = criterion(logits_per_text, ground_truth)
# total_loss = 0.5 * (loss_img + loss_txt)
# total_loss.backward()
# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
# optimizer.step() # Update weights

# def evaluate(model, test_loader, criterion, device:str="cuda"):
# 	model.eval()
# 	total_loss = 0
# 	total_correct_text_description_for_each_image = 0
# 	total_correct_image_for_each_text_description = 0
# 	with torch.no_grad():
# 		for batch_idx, (images, labels) in enumerate(test_loader):
# 			images, labels = images.to(device), labels.to(device)
# 			logits_per_image, logits_per_text = model(images, labels) # torch.Size([batch_size, batch_size]) torch.Size([batch_size, batch_size])
# 			_, predicted_idxs_imgs = torch.max(input=logits_per_image, dim=1, keepdim=True)
# 			_, predicted_idxs_txts = torch.max(input=logits_per_text, dim=1, keepdim=True)
# 			correct_text_description_idxs = torch.argmax(labels, dim=1) # indices of correct text descriptions for each image
# 			# Compare predicted indexes with the correct indexes
# 			total_correct_text_description_for_each_image += (predicted_idxs_imgs == correct_text_description_idxs.unsqueeze(1)).sum().item()
# 			total_correct_image_for_each_text_description += (predicted_idxs_txts == correct_text_description_idxs.unsqueeze(1)).sum().item()

# 			# validation loss
# 			ground_truth = torch.arange(start=0, end=len(images), dtype=torch.long, device=device)
# 			loss_img = criterion(logits_per_image, ground_truth) 
# 			loss_txt = criterion(logits_per_text, ground_truth)
# 			valid_loss = 0.5 * (loss_img + loss_txt)
# 			total_loss += valid_loss.item()
# 	avg_loss = total_loss / len(test_loader)
# 	accuracy_text_description_for_each_image = total_correct_text_description_for_each_image / len(test_loader.dataset)
# 	accuracy_text_image_for_each_text_description = total_correct_image_for_each_text_description / len(test_loader.dataset)
# 	return avg_loss, accuracy_text_description_for_each_image, accuracy_text_image_for_each_text_description


more advanced:
def evaluate(model, test_loader, criterion, device="cuda"):
	model.eval()
	total_loss = 0
	correct_text_description = 0
	correct_image_for_text = 0
	total_samples = 0
	with torch.no_grad():
		for bidx, (images, labels) in enumerate(test_loader):
			images, labels = images.to(device), labels.to(device)
			batch_size = images.size(0)
			total_samples += batch_size
			logits_per_image, logits_per_text = model(images, labels) # torch.Size([batch_size, batch_size]) torch.Size([batch_size, batch_size])
			# Predictions
			predicted_text_idxs = torch.argmax(input=logits_per_image, dim=1) # indices of maximum value of all elements in input tensor. torch.Size([batch_size])
			predicted_image_idxs = torch.argmax(input=logits_per_text, dim=1)
			correct_labels = torch.arange(start=0, end=batch_size, dtype=torch.long, device=device) # ground truth labels for each batch item torch.Size([batch_size])
			# Metrics
			correct_text_description += (predicted_text_idxs == correct_labels).sum().item()
			correct_image_for_text += (predicted_image_idxs == correct_labels).sum().item()
			# Validation loss
			loss_img = criterion(logits_per_image, correct_labels)
			loss_txt = criterion(logits_per_text, correct_labels)
			total_loss += 0.5 * (loss_img.item() + loss_txt.item())
	# Compute average loss and accuracies
	avg_loss = total_loss / len(test_loader)
	accuracy_text_description = correct_text_description / total_samples
	accuracy_image_for_text = correct_image_for_text / total_samples
	return avg_loss, accuracy_text_description, accuracy_image_for_text


###################################################################################
# GPU cosine similarity + Average recommendation vector:
def get_customized_cosine_similarity_gpu(spMtx, query_vec, idf_vec, spMtx_norm, exponent:float=1.0, batch_size:int=2048):
		print(f"[GPU Optimized] Customized Cosine Similarity (1 x nUsers={spMtx.shape[0]}) batch_size={batch_size}".center(130, "-"))
		print(
			f"Query: {query_vec.shape} {type(query_vec)} {query_vec.dtype} non_zeros={np.count_nonzero(query_vec)} (ratio={np.count_nonzero(query_vec) / query_vec.size})\n"
			f"spMtx {type(spMtx)} {spMtx.shape} {spMtx.dtype}\n"
			f"spMtxNorm: {type(spMtx_norm)} {spMtx_norm.shape} {spMtx_norm.dtype}\n"
			f"IDF {type(idf_vec)} {idf_vec.shape} {idf_vec.dtype}"
		)
		# Clear memory before starting
		cp.get_default_memory_pool().free_all_blocks()
		torch.cuda.empty_cache()
		
		# Print GPU device information
		device = cp.cuda.Device()
		device_id = device.id
		device_name = cp.cuda.runtime.getDeviceProperties(device_id)['name'].decode('utf-8')
		print(f"GPU: {device_name} ({device_id})")
		print(f"Initial Free GPU Memory: {device.mem_info[0] / 1024 ** 3:.2f} GB / Total GPU Memory: {device.mem_info[1] / 1024 ** 3:.2f} GB")

		st_t = time.time()
		# Convert inputs to CuPy arrays (float32)
		query_vec_squeezed = cp.asarray(query_vec.ravel(), dtype=cp.float32)
		idf_squeezed = cp.asarray(idf_vec.ravel(), dtype=cp.float32)
		spMtx_norm = cp.asarray(spMtx_norm, dtype=cp.float32)

		# Convert sparse matrix to CuPy CSR format
		spMtx_csr = spMtx.tocsr()
		spMtx_gpu = cp.sparse.csr_matrix(
				(cp.asarray(spMtx_csr.data, dtype=cp.float32), cp.asarray(spMtx_csr.indices), cp.asarray(spMtx_csr.indptr)),
				shape=spMtx_csr.shape
		)

		# Compute quInterest and its norm
		quInterest = query_vec_squeezed * idf_squeezed
		quInterestNorm = cp.linalg.norm(quInterest)

		# Get indices of non-zero elements in quInterest
		idx_nonzeros = cp.nonzero(quInterest)[0]
		quInterest_nonZeros = quInterest[idx_nonzeros] / quInterestNorm

		# Normalize user interests
		usrInterestNorm = spMtx_norm + cp.float32(1e-4)

		# Initialize result array
		cs = cp.zeros(spMtx_gpu.shape[0], dtype=cp.float32)

		# Process in batches to avoid memory overflow
		for i in range(0, spMtx_gpu.shape[0], batch_size):
				# Define batch range
				start_idx = i
				end_idx = min(i + batch_size, spMtx_gpu.shape[0])

				# Extract batch from sparse matrix
				spMtx_batch = spMtx_gpu[start_idx:end_idx, :]

				# Extract only the necessary columns from the batch
				spMtx_nonZeros = spMtx_batch[:, idx_nonzeros]

				# Apply IDF and normalize
				spMtx_nonZeros = spMtx_nonZeros.multiply(idf_squeezed[idx_nonzeros])
				spMtx_nonZeros = spMtx_nonZeros.multiply(1 / usrInterestNorm[start_idx:end_idx, None])

				# Apply exponent if necessary
				if exponent != 1.0:
						spMtx_nonZeros.data **= exponent

				# Compute cosine similarity scores for the batch
				cs_batch = spMtx_nonZeros.dot(quInterest_nonZeros)

				# Store batch results
				cs[start_idx:end_idx] = cs_batch

				# Free memory for the batch
				del spMtx_batch, spMtx_nonZeros, cs_batch
				cp.get_default_memory_pool().free_all_blocks()
				torch.cuda.empty_cache() # Clear CUDA cache
				# torch.cuda.synchronize() # Ensure all CUDA operations are complete
				# Print memory usage after each batch
				# print(f"Batch {i // batch_size + 1}: Free GPU Memory: {device.mem_info[0] / 1024 ** 3:.2f} GB")

		print(f"Elapsed_t: {time.time() - st_t:.2f} s {type(cs)} {cs.dtype} {cs.shape}".center(130, " "))
		return cp.asnumpy(cs)  # Convert result back to NumPy for compatibility

def get_customized_recsys_avg_vec_gpu(spMtx, cosine_sim, idf_vec, spMtx_norm, batch_size:int=2048):
		print(f"[GPU optimized] avgRecSys (1 x nTKs={spMtx.shape[1]})".center(130, "-"))
		st_t = time.time()
		
		# Move data to GPU
		idf_squeezed = cp.asarray(idf_vec.ravel(), dtype=cp.float32)
		cosine_sim_gpu = cp.asarray(cosine_sim, dtype=cp.float32)
		spMtx_norm_gpu = cp.asarray(spMtx_norm, dtype=cp.float32)
		
		# Find non-zero cosine similarities
		non_zero_cosines = cp.nonzero(cosine_sim_gpu)[0]
		non_zero_values = cosine_sim_gpu[non_zero_cosines]
		
		print(
				f"spMtx {type(spMtx)} {spMtx.shape} {spMtx.dtype}\n"
				f"spMtxNorm: {type(spMtx_norm)} {spMtx_norm.shape} {spMtx_norm.dtype}\n"
				f"CS {type(cosine_sim)} {cosine_sim.shape} {cosine_sim.dtype} NonZero(s): {len(non_zero_cosines)}\n"
				f"IDF {type(idf_vec)} {idf_vec.shape} {idf_vec.dtype}"
		)
		
		# Convert sparse matrix to CuPy CSR format
		spMtx_csr = spMtx.tocsr()
		spMtx_gpu = cp.sparse.csr_matrix(
				(cp.asarray(spMtx_csr.data, dtype=cp.float32),
				 cp.asarray(spMtx_csr.indices),
				 cp.asarray(spMtx_csr.indptr)),
				shape=spMtx_csr.shape
		)
		
		# Initialize result array on GPU
		avg_rec = cp.zeros(spMtx.shape[1], dtype=cp.float32)
		
		# Process in batches
		for i in range(0, len(non_zero_cosines), batch_size):
				batch_indices = non_zero_cosines[i:i + batch_size]
				batch_values = non_zero_values[i:i + batch_size]
				
				# Extract batch from sparse matrix
				spMtx_batch = spMtx_gpu[batch_indices]
				
				# Apply IDF
				batch_result = spMtx_batch.multiply(idf_squeezed)
				
				# Normalize by user interest norm
				norm_factors = spMtx_norm_gpu[batch_indices] + cp.float32(1e-18)
				batch_result = batch_result.multiply(1.0 / norm_factors[:, None])
				
				# Multiply by cosine similarities
				batch_result = batch_result.multiply(batch_values[:, None])
				
				# Add to running sum
				avg_rec += batch_result.sum(axis=0).ravel()
				
				# Clean up memory
				del batch_result, spMtx_batch
				cp.get_default_memory_pool().free_all_blocks()
		
		# Normalize the result
		avg_rec /= cp.sum(non_zero_values)
		
		# Convert back to CPU
		result = cp.asnumpy(avg_rec)
		
		print(f"Elapsed_t: {time.time()-st_t:.2f} s {type(result)} {result.dtype} {result.shape}".center(130, " "))
		return result


:root {
	--primary-color: #5b37b1;
	--spacing-unit: 8px;
}

body {
	display: flex;
	flex-direction: column;
	height: 100vh;
	max-width: 1200px; /* Ensure max-width for consistency on all screens */
	margin: 0 auto; /* centered layout */
	padding: 0;
	box-sizing: border-box;
}

@keyframes glow {
	from {
		text-shadow: 
			0 0 10px #5827cab0, 
			0 0 20px #ebff78, 
			0 0 40px #edf5c8, 
			0 0 80px #d4df98;
	}
	
	to {
		text-shadow: 
			0 0 10px rgb(50, 180, 180), 
			0 0 30px #81adf0, 
			0 0 50px #c699f0, 
			0 0 50px #dbbacb,
			0 0 90px #e4d2db;
	}
}

.glow {
	color: #6b47c0b0;
	font-weight: bold;
	font-family: 'Poppins', sans-serif;
	text-align: center;
	font-size: 2rem;
	margin-top: 1rem;
	margin-bottom: 1rem;
	animation: glow 0.85s ease-in-out infinite alternate; /* Slows the animation */
}

.navbar {
	display: flex;
	align-items: center;
	justify-content: flex-start;
	background-color: #333;
	padding: 10px 20px;
	border-radius: 0 0 8px 8px;
}

.navbar a {
	text-decoration: none;
	color: white;
	font-size: 1.15rem;
	padding: 12px 20px;
	margin-right: 15px;
	transition: background-color 0.3s ease; /* Smooth transition */
}

.navbar a.home {
	background-color: #5b37b1;
	font-weight: bold;
	border-radius: 4px;
}

.navbar a:hover {
	background-color: #575757;
	color: white;
}

.container {
	text-align: center;
	transition: transform 0.5s ease, width 0.5s ease;
	width: 100%; /* Ensure full width for proper translation */
}

.container.translated {
	transform: translateX(-30%);
	width: 70%;
}

.imageContainer {
	display: flex;
	justify-content: center;
}

.imageContainer img {
	width: 100%; /* Full width, will scale naturally */
	max-width: 200px; /* Prevents images from growing too large */
	height: auto; /* Maintain aspect ratio */
	margin: 10px;
	filter: grayscale(100%);
}

.search-container {
	width: 100%;
	justify-content: center;
	/* background-color: #ffdac1d8; */
	align-items: center;
}

.search-container h2 {
	font-size: 25px;
	font-weight: bold;
	color: #000;
	margin-bottom: 5px;
}

.search-container h3 {
	color: #000;
	margin-bottom: 20px;
}

.search-form {
	justify-content: center;
	position: relative;
	height: 380px;
}

.search-form::before {
	content: "";
	position: absolute;
	top: 0;
	right: 0;
	bottom: 0;
	left: 0;
	background-image: url("https://aptitude-test.com/wp-content/uploads/2023/05/pic.jpg");
	background-size: 64% 100%;
	background-repeat: no-repeat;
	background-position: center top 0px;
	filter: grayscale(0.8);
	z-index: -1;
}

.search-input {
	position: relative;
}

.search-input-field {
	width: 60%; /* Fixed width that works well on different screen sizes */
	height: 25px;
	font-size: 1.35rem;
	padding: 10px;
	font-weight: bold;
	font-family: Georgia, 'Times New Roman', Times, serif;
	border-radius: 8px;
	border: none;
	background-color: #e1e2e2;
	margin-top: 18px;
	caret-color: rgb(26, 250, 201);
}

.search-input-field:focus {
	background-color: #ffffff;
	color: #303030c5;
	border: 2px solid #080808;
}

.help-container {
	width: 32%;
	height: 75%;
	font-size: 16px;
	font-weight: bold;
	position: absolute;
	top: 0;
	right: 0;
	background: transparent url("https://i.pinimg.com/564x/1c/f7/80/1cf7809521b1fc112c8b116ccb1e2a01.jpg") no-repeat scroll center;
	background-size: 180px 50px;
	display: flex;
	justify-content: center;
	align-items: center;
	display: none; /* Initially hidden */
}

.search-input-field:focus + .help-container {
	display: flex;
	justify-content: center;
	align-items: center;
	text-decoration: none;
	z-index: 1;
}

.fold {
	width: 45%;
	height: 300px;
	border-radius: 15px;
	color: #0c0c0cc4;
	position: absolute;
	left: calc(54% + 0px);
	top: calc(80% + 0px);
	text-align: left;
	padding: 10px;
	background: -webkit-linear-gradient(top, #e6e6e6e7, #d1b5fd93);
	font-size: 15px;
	font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif;
	transition: all 0.7s ease;
}

.unfolder { 
	display: none;
}

.toggle-label {
	display: inline-block;
	cursor: pointer;
}

.unfold-icon, .fold-icon {
	color: #7b47db;
	width: 10px;
	display: inline-block;
}

.unfolder ~ .fold {
	display: none;
}

.unfolder ~ label .fold-icon {
	display: none;
}

.unfolder:checked ~ .fold {
	display: block;
}

.unfolder:checked ~ label .fold-icon {
	display: inline-block;
}

.unfolder:checked ~ label .unfold-icon {
	display: none;
}

.button-container {
	display: flex;
	justify-content: center;
	align-items: center;
	gap: 25px;
	margin: 25px;
}

.button-container.vertical {
	flex-direction: column;
}

.btn-nlf-search:hover {
	background-color: rgba(0, 3, 200, 0.8);
	color: #e2e0e0;
	font-weight: bold;
}

.btn-clear:hover {
	background-color: rgba(200, 1, 0, 0.8);
	color: #e2e0e0;
	font-weight: bold;
}

input[type="submit"]:hover {
	background-color: rgba(5, 116, 8, 0.8);
	color: #e2e0e0;
	font-weight: bold;
}

.btn-nlf-search, .btn-clear, input[type="submit"] {
	width: clamp(100px, 10vw, 150px);
	height: 35px;
	font-size: 18px;
	font-weight: normal;
	border-radius: 15px;
	margin: 2px 0;
	transition: all 0.3s ease;
	background-color: rgb(149, 145, 145);
	color:#000;
	border: none;
	outline: none;
	cursor: pointer;
	font-family: 'Times New Roman', Times, serif;
}

#libraryLinkContainer {
	font-size: 22px;
	font-weight: bold;
	color: rgb(1, 10, 250);
	font-family: 'Times New Roman', Times, serif;
}

.blur-background {
	backdrop-filter: invert(80%);
	display: inline-block;
	padding: 10px;
	background-color: #9b9b9bc7;
}

.slider-container {
	margin: 10px;
	text-align: center;
}

#recSysSlider {
	-webkit-appearance: none;
	appearance: none;
	width: 32%;
	height: 10px;
	background: rgb(180, 180, 180);
	outline: none;
	opacity: 0.4;
	-webkit-transition: .4s;
	transition: opacity .4s;
	border-radius: 9px;
}

#recSysSliderLbl {
	font-size: 16px;
	font-family: 'Times New Roman', Times, serif;
	font-style: oblique;
	background-color: #f0ec00;
}

#recSysSlider:hover {
	opacity: 1;
}

#recSysSlider::-webkit-slider-thumb {
	-webkit-appearance: none;
	appearance: none;
	width: 20px;
	height: 20px;
	background: #07880c;
	cursor: pointer;
	border-radius: 50%;
}

#recSysSlider::-moz-range-thumb {
	width: 20px;
	height: 20px;
	background: #4CAF50;
	cursor: pointer;
	border-radius: 50%;
}

.loading-container {
	display: flex;
	flex-direction: column;
	align-items: center;	
}

.spinner-text {
	color: #ffffffe8;
	font-family: 'Times New Roman', Times, serif;
	font-size: 25px;
	font-weight: bold;
	font-style: oblique;
	backdrop-filter: blur(5px);
	background-color: rgba(179, 179, 179, 0.644);
	-webkit-backdrop-filter: blur(5px);
}

.loading-spinner {
	display: none;
}

.loading-spinner:before {
	content: '';
	box-sizing: border-box;
	position: absolute;
	width: 65px;
	height: 65px;
	margin-left: -70px;
	border-radius: 70%;
	border: 5px solid #e0e0e0;
	border-top: 1px solid transparent;
	animation: spin 0.7s linear infinite;
}

@keyframes spin {
	0% { transform: rotate(0deg); }
	100% { transform: rotate(360deg); }
}

.feedback-option, .feedback-label {
	width: 100px; /* Set a fixed width for both columns */
	text-align: center; /* Center-align the content */
	font-weight: bold; /* Make the text bold */
}

/* Recommendation Table CSS layout */
/* ##################################### */
.recommendationsContainer {
	display: flex;
	flex-direction: column;
	align-items: center;
	justify-content: center;
	background-color: #ffffff; /* MUST BE SET TO WHITE*/
}

#recSysIntroContainer {
	color: rgba(1, 20, 14, 0.6);
	font-weight: bold;
	font-size: 22px;
}

#recommendationTable {
	width: 100%; /* Keep the table consistent in width */
	border-collapse: collapse; /* Collapse borders between cells */
	margin: 0 auto;
	font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif;
}

#recommendationTable th {
	background-color: #222222;
	color: white;
	padding: 8px;
	text-align: center;
	position: sticky; /* Make the header sticky */
	top: 0; /* Stick to the top of the container */
	z-index: 2; /* Ensure it stays above other content */
}

#recommendationTable tr {
	font-size: 25px;
	background-color: #bebebebe; /* Light gray background for rows */
}

#recommendationTable tr:nth-child(even) {
	background-color: #747474ab; /* Darker gray for even rows */
}

#recommendationTable td {
	padding: 41px; /* must be padding for adjuctment of text, box and chart*/
	border: 1px solid #dadada; /* Light gray border around cells */
	text-align: left;
}

.rec-link {
	display: inline-block;
	vertical-align: middle;
	text-align: left;
	transition: all 0.3s ease;
}

/* 
#####For any presentations, it should be uncommented!#####
#recommendationResultContainer tr:hover .rec-link {
	font-size: 1.15em;
	line-height: 2.8;
	background-color: rgba(223, 223, 223, 0.93);
	color: #001cb9;
	padding: 1px;
	border-radius: 5px;
	position: relative;
	z-index: 1;
}

#recommendationResultContainer tr:hover .rec-link::before {
	content: "";
	position: absolute;
	top: 0;
	left: 0;
	right: 0;
	bottom: 0;
	background: rgba(180, 180, 180, 0.815);
	filter: blur(8px);
	z-index: -1;
	border-radius: 8px;
}

#recommendationResultContainer tr:hover {
	background-color: inherit;
} 
#####For any presentations, it should be uncommented!#####
*/

tbody tr {
	position: relative; /* table row position is relative */
}

td:first-child {
	position: relative;
	padding-right: 50px;
	text-align: left;
}

.pie-chart-container {
	display: inline-block;
	width: 90px;
	height: 90px;
	position: absolute;
	top: 50%;
	transform: translateY(-50%);
	right: 120px; /* Position it to the left of the circular box */
}

.pieChart {
	width: 100%;
	height: 100%;
}

.pieSegment {
	transition: transform 0.2s;
	transform-origin: center;
	/* stroke: black;
	stroke-width: 1px; */
}

.pieSegment:hover {
	transform: scale(1.02);
	filter: brightness(1.09);
	/* stroke-width: 2px; */
}

#recommendationResultContainer tr:hover .pie-chart-container {
	transform: translateY(-50%) scale(1.6);
	transition: transform 0.3s ease-in-out;
}

.pie-chart-legend-container {
	display: flex;
	align-items: center;
	justify-content: center;
	margin-top: 10px;
	color: #383838;
	/* font-style: oblique; */
}

.legend-container {
	display: flex;
	justify-content: center;
}

.legend-item {
	display: flex;
	align-items: center;
	margin: 0 15px;
}

.legend-color {
	width: 25px;
	height: 25px;
	border-radius: 50%;
	margin-right: 5px;
}

.legend-text {
	color: #e9e9e9;
	font-weight: bold;
	font-size: 17px;
}


/* Responsive scaling based on screen size using media queries */
@media screen and (max-width: 768px) {
	.button-container.vertical {
			right: -100px;
			top: 40%;
	}

	/* .button-container {
		gap: 15px;
	} */

	.btn-nlf-search, .btn-clear, input[type="submit"] {
			width: 100px;
			font-size: 14px;
	}
}

@media screen and (max-width: 480px) {
	.button-container.vertical {
			right: -80px;
			top: 30%;
	}
	
	.btn-nlf-search, .btn-clear, input[type="submit"] {
			width: 80px;
			font-size: 12px;
	}
}




/* Responsive scaling based on screen size using media queries */
@media screen and (max-width: 1200px) {
	.body {
		max-width: 100%;
		padding: 0 15px; /* Add padding for smaller screens */
	}
}

@media screen and (max-width: 768px) {
	.navbar a {
		font-size: 1.6rem;
	}


	/* .btn-nlf-search, .btn-clear, input[type="submit"] {
		width: 120px;
		height: 35px;
		font-size: 1rem;
		transition: all 0.3s ease;
	} */

	.pie-chart-container {
		width: 60px;
		height: 60px;
		right: 50px;
	}
}

@media screen and (max-width: 480px) {
	.search-input-field {
		width: 80%; /* Expand the input for smaller screens */
	}

	.pie-chart-container {
		width: 50px;
		height: 50px;
		right: 30px;
	}
}

.circular-box {
	display: inline-block;
	width: 75px;
	height: 40px;
	line-height: 40px;
	border-radius: 8%;
	background-color: #021064;
	color: #ffffff;
	font-size: 18px;
	font-weight: bold;
	text-align: center;
	position: absolute;
	top: 50%;
	transform: translateY(-50%);
	right: 10px;
}